{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8349a7b",
   "metadata": {},
   "source": [
    "# Lab 4: Dropout, Gradient Clipping & Multitask w/ EarlyStopping\n",
    "\n",
    "Simplified and cleaned code from your lab manual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d7a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 4a: Dropout & Gradient Clipping demo (synthetic data)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_dropout_model(dropout_rate=0.2):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "dropout_model = create_dropout_model(0.2)\n",
    "dropout_history = dropout_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "def create_gradient_clip_model(clip_norm=1.0):\n",
    "    model = Sequential([\n",
    "        Dense(64, input_shape=(X_train.shape[1],), activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    optimizer = Adam(clipnorm=clip_norm)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "gradient_clip_model = create_gradient_clip_model(clip_norm=1.0)\n",
    "gradient_clip_history = gradient_clip_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Plot accuracies\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(dropout_history.history['accuracy'], label='Dropout Train Acc', linestyle='--')\n",
    "plt.plot(dropout_history.history['val_accuracy'], label='Dropout Val Acc')\n",
    "plt.plot(gradient_clip_history.history['accuracy'], label='GradClip Train Acc', linestyle='--')\n",
    "plt.plot(gradient_clip_history.history['val_accuracy'], label='GradClip Val Acc')\n",
    "plt.title('Training and Validation Accuracy'); plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend(); plt.grid(True); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b611de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 4b: Multitask learning with EarlyStopping on MNIST (parity task)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load MNIST and create parity auxiliary task (even/odd)\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0; x_test = x_test / 255.0\n",
    "y_train_parity = np.array([np.sum(list(map(int, str(y)))) % 2 for y in y_train])\n",
    "y_test_parity = np.array([np.sum(list(map(int, str(y)))) % 2 for y in y_test])\n",
    "\n",
    "# Display sample images\n",
    "plt.figure(figsize=(6,6))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1); plt.xticks([]); plt.yticks([]); plt.grid(False)\n",
    "    plt.imshow(x_train[i], cmap='gray'); plt.xlabel(y_train[i])\n",
    "plt.show()\n",
    "\n",
    "inputs = keras.Input(shape=(28,28))\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax')(x)\n",
    "outputs_parity = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(inputs=inputs, outputs=[outputs, outputs_parity])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=['sparse_categorical_crossentropy', 'binary_crossentropy'],\n",
    "              metrics=[['accuracy'], ['accuracy']])\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(x_train, [y_train, y_train_parity], epochs=20, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "# Determine early stopping epoch (approx)\n",
    "val_loss = history.history.get('val_loss', [])\n",
    "if val_loss:\n",
    "    early_epoch = np.argmin(val_loss) + 1\n",
    "else:\n",
    "    early_epoch = None\n",
    "\n",
    "print('Early stopping occurred at epoch:', early_epoch)\n",
    "\n",
    "# Plot training & validation loss with early stopping marker\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "if early_epoch:\n",
    "    plt.axvline(x=early_epoch, color='r', linestyle='--', label='Early Stopping')\n",
    "plt.xlabel('Epochs'); plt.ylabel('Loss'); plt.title('Training & Validation Loss'); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Lab 4"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
